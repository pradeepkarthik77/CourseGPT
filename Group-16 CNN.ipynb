{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XccAvA-Puo5",
        "outputId": "119dfb9f-4cbd-41e0-de12-02480630f54c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "89/89 [==============================] - 59s 641ms/step - loss: 8.1524 - accuracy: 0.0011 - val_loss: 8.1734 - val_accuracy: 0.0014\n",
            "Epoch 2/10\n",
            "89/89 [==============================] - 56s 627ms/step - loss: 8.1220 - accuracy: 0.0018 - val_loss: 8.2413 - val_accuracy: 0.0014\n",
            "Epoch 3/10\n",
            "89/89 [==============================] - 56s 636ms/step - loss: 8.0908 - accuracy: 0.0018 - val_loss: 8.3571 - val_accuracy: 0.0014\n",
            "Epoch 4/10\n",
            "89/89 [==============================] - 57s 636ms/step - loss: 7.9522 - accuracy: 0.0014 - val_loss: 8.6125 - val_accuracy: 0.0014\n",
            "Epoch 5/10\n",
            "89/89 [==============================] - 56s 633ms/step - loss: 7.6397 - accuracy: 0.0046 - val_loss: 9.3570 - val_accuracy: 0.0043\n",
            "Epoch 6/10\n",
            "89/89 [==============================] - 59s 662ms/step - loss: 7.1240 - accuracy: 0.0085 - val_loss: 10.4774 - val_accuracy: 0.0128\n",
            "Epoch 7/10\n",
            "89/89 [==============================] - 62s 696ms/step - loss: 6.4170 - accuracy: 0.0259 - val_loss: 11.6436 - val_accuracy: 0.0128\n",
            "Epoch 8/10\n",
            "89/89 [==============================] - 57s 641ms/step - loss: 5.4342 - accuracy: 0.0703 - val_loss: 13.7242 - val_accuracy: 0.0199\n",
            "Epoch 9/10\n",
            "89/89 [==============================] - 60s 670ms/step - loss: 4.2159 - accuracy: 0.2059 - val_loss: 15.3043 - val_accuracy: 0.0340\n",
            "Epoch 10/10\n",
            "89/89 [==============================] - 57s 647ms/step - loss: 3.0420 - accuracy: 0.3944 - val_loss: 16.4879 - val_accuracy: 0.0383\n",
            "23/23 [==============================] - 3s 118ms/step\n",
            "Top 20 Recommended Courses:\n",
            "Finance for Managers\n",
            "Retrieve Data using Single-Table SQL Queries\n",
            "The Roles and Responsibilities of Nonprofit Boards of Directors within the Governance Process\n",
            "Global Health: An Interdisciplinary Overview\n",
            "Python Programming Essentials\n",
            "Parallel programming\n",
            "Multiple Regression Analysis in Public Health\n",
            "Philosophy, Science and Religion: Philosophy and Religion\n",
            "Protecting Business Innovations via Patent\n",
            "Introduction to Cybersecurity Tools & Cyber Attacks\n",
            "Introduction to Cybersecurity Tools & Cyber Attacks\n",
            "Prediction and Control with Function Approximation\n",
            "Python Tricks and Hacks for Productivity\n",
            "Introduction to Recommender Systems:  Non-Personalized and Content-Based\n",
            "Predictive Modeling and Analytics\n",
            "Genetics and Society: A Course for Educators\n",
            "Russian History: from Lenin to Putin\n",
            "Successful Interviewing\n",
            "Mastering SQL Joins\n",
            "Building Resilient Streaming Analytics Systems on GCP\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, concatenate\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Load the dataset\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/ML/Coursera.csv')\n",
        "\n",
        "# Preprocess the data\n",
        "X_desc = dataset['Course Description'].values\n",
        "X_name = dataset['Course Name'].values\n",
        "X_skills = dataset['Skills'].values\n",
        "y = dataset['Course Name'].values\n",
        "\n",
        "# Encode the target variable\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_desc)\n",
        "X_desc = tokenizer.texts_to_sequences(X_desc)\n",
        "X_desc = pad_sequences(X_desc)\n",
        "\n",
        "tokenizer.fit_on_texts(X_name)\n",
        "X_name = tokenizer.texts_to_sequences(X_name)\n",
        "X_name = pad_sequences(X_name)\n",
        "\n",
        "tokenizer.fit_on_texts(X_skills)\n",
        "X_skills = tokenizer.texts_to_sequences(X_skills)\n",
        "X_skills = pad_sequences(X_skills)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_desc_train, X_desc_test, X_name_train, X_name_test, X_skills_train, X_skills_test, y_train, y_test = train_test_split(\n",
        "    X_desc, X_name, X_skills, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define input layers\n",
        "input_desc = Input(shape=(X_desc.shape[1],))\n",
        "input_name = Input(shape=(X_name.shape[1],))\n",
        "input_skills = Input(shape=(X_skills.shape[1],))\n",
        "\n",
        "# Embedding layers\n",
        "embedding_desc = Embedding(vocab_size, 100)(input_desc)\n",
        "embedding_name = Embedding(vocab_size, 100)(input_name)\n",
        "embedding_skills = Embedding(vocab_size, 100)(input_skills)\n",
        "\n",
        "# Convolutional layers for each input\n",
        "conv_desc = Conv1D(128, 5, activation='relu')(embedding_desc)\n",
        "conv_name = Conv1D(128, 5, activation='relu')(embedding_name)\n",
        "conv_skills = Conv1D(128, 5, activation='relu')(embedding_skills)\n",
        "\n",
        "# Global Max Pooling layers\n",
        "pool_desc = GlobalMaxPooling1D()(conv_desc)\n",
        "pool_name = GlobalMaxPooling1D()(conv_name)\n",
        "pool_skills = GlobalMaxPooling1D()(conv_skills)\n",
        "\n",
        "# Concatenate the pooled features\n",
        "concatenated = concatenate([pool_desc, pool_name, pool_skills])\n",
        "\n",
        "# Dense layers\n",
        "dense = Dense(128, activation='relu')(concatenated)\n",
        "dropout = Dropout(0.2)(dense)\n",
        "output = Dense(len(le.classes_), activation='softmax')(dropout)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=[input_desc, input_name, input_skills], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit([X_desc_train, X_name_train, X_skills_train], y_train, validation_data=([X_desc_test, X_name_test, X_skills_test], y_test), epochs=10, batch_size=32)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict([X_desc_test, X_name_test, X_skills_test])\n",
        "\n",
        "input = \"I am a SQL developer but I need to use this in the financial sector and I need to aware of SQL injections for cybersecuirty can you give a course?\"\n",
        "\n",
        "courses = model.predict(input)\n",
        "\n",
        "# Convert predictions back to original labels\n",
        "predicted_labels = le.inverse_transform(np.argmax(predictions, axis=1))\n",
        "\n",
        "# Retrieve top 20 recommended courses\n",
        "top_20_courses = dataset[dataset['Course Name'].isin(predicted_labels)]['Course Name'].head(20).values\n",
        "\n",
        "# Print the top 20 recommended courses\n",
        "print(\"Top 20 Recommended Courses:\")\n",
        "for course in top_20_courses:\n",
        "    print(course)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CL3ZeYvtuLn",
        "outputId": "88dc6eb0-840e-40f6-e630-db7e7cd1cdeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "89/89 [==============================] - 53s 580ms/step - loss: 8.1514 - accuracy: 3.5499e-04 - val_loss: 8.1733 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/2\n",
            "89/89 [==============================] - 56s 631ms/step - loss: 8.1213 - accuracy: 0.0018 - val_loss: 8.2558 - val_accuracy: 0.0014\n",
            "1/1 [==============================] - 0s 174ms/step\n",
            "Top 20 Recommended Courses:\n",
            "What is Data Science?\n",
            "What is Data Science?\n",
            "What is Data Science?\n",
            "What is Data Science?\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, concatenate\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Load the dataset\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/ML/Coursera.csv')\n",
        "\n",
        "# Preprocess the data\n",
        "X_desc = dataset['Course Description'].values\n",
        "X_name = dataset['Course Name'].values\n",
        "X_skills = dataset['Skills'].values\n",
        "y = dataset['Course Name'].values\n",
        "\n",
        "# Encode the target variable\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_desc)\n",
        "X_desc = tokenizer.texts_to_sequences(X_desc)\n",
        "X_desc = pad_sequences(X_desc)\n",
        "\n",
        "tokenizer.fit_on_texts(X_name)\n",
        "X_name = tokenizer.texts_to_sequences(X_name)\n",
        "X_name = pad_sequences(X_name)\n",
        "\n",
        "tokenizer.fit_on_texts(X_skills)\n",
        "X_skills = tokenizer.texts_to_sequences(X_skills)\n",
        "X_skills = pad_sequences(X_skills)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_desc_train, X_desc_test, X_name_train, X_name_test, X_skills_train, X_skills_test, y_train, y_test = train_test_split(\n",
        "    X_desc, X_name, X_skills, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define input layers\n",
        "input_desc = Input(shape=(X_desc.shape[1],))\n",
        "input_name = Input(shape=(X_name.shape[1],))\n",
        "input_skills = Input(shape=(X_skills.shape[1],))\n",
        "\n",
        "# Embedding layers\n",
        "embedding_desc = Embedding(vocab_size, 100)(input_desc)\n",
        "embedding_name = Embedding(vocab_size, 100)(input_name)\n",
        "embedding_skills = Embedding(vocab_size, 100)(input_skills)\n",
        "\n",
        "# Convolutional layers for each input\n",
        "conv_desc = Conv1D(128, 5, activation='relu')(embedding_desc)\n",
        "conv_name = Conv1D(128, 5, activation='relu')(embedding_name)\n",
        "conv_skills = Conv1D(128, 5, activation='relu')(embedding_skills)\n",
        "\n",
        "# Global Max Pooling layers\n",
        "pool_desc = GlobalMaxPooling1D()(conv_desc)\n",
        "pool_name = GlobalMaxPooling1D()(conv_name)\n",
        "pool_skills = GlobalMaxPooling1D()(conv_skills)\n",
        "\n",
        "# Concatenate the pooled features\n",
        "concatenated = concatenate([pool_desc, pool_name, pool_skills])\n",
        "\n",
        "# Dense layers\n",
        "dense = Dense(128, activation='relu')(concatenated)\n",
        "dropout = Dropout(0.2)(dense)\n",
        "output = Dense(len(le.classes_), activation='softmax')(dropout)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=[input_desc, input_name, input_skills], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit([X_desc_train, X_name_train, X_skills_train], y_train, validation_data=([X_desc_test, X_name_test, X_skills_test], y_test), epochs=2, batch_size=32)\n",
        "\n",
        "# Preprocess input\n",
        "def preprocess_input(input_string):\n",
        "    input_desc = tokenizer.texts_to_sequences([input_string])\n",
        "    input_desc = pad_sequences(input_desc, maxlen=X_desc.shape[1])\n",
        "    input_name = pad_sequences([[0]], maxlen=X_name.shape[1])  # Pad dummy input with zeros\n",
        "    input_skills = pad_sequences([[0]], maxlen=X_skills.shape[1])  # Pad dummy input with zeros\n",
        "    return input_desc, input_name, input_skills\n",
        "\n",
        "# Get preprocessed input for recommendation\n",
        "input_string = \"Python coding business economics data science\"\n",
        "input_desc_recommend, input_name_recommend, input_skills_recommend = preprocess_input(input_string)\n",
        "\n",
        "# Make recommendations\n",
        "recommendations = model.predict([input_desc_recommend, input_name_recommend, input_skills_recommend])\n",
        "recommended_labels = le.inverse_transform(np.argmax(recommendations, axis=1))\n",
        "\n",
        "# Retrieve top 20 recommended courses\n",
        "top_20_courses = dataset[dataset['Course Name'].isin(recommended_labels)]['Course Name'].head(20).values\n",
        "\n",
        "# Print the top 20 recommended courses\n",
        "print(\"Recommended Courses:\")\n",
        "# for course in top_20_courses:\n",
        "for i in range(4):\n",
        "    print(top_20_courses[0])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
